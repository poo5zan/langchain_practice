{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwTwsobGPvigjARxE16rM+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poo5zan/langchain_practice/blob/main/langchain_pujan_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rCbWIsrLvPZ"
      },
      "outputs": [],
      "source": [
        "# Run ollama in google colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n"
      ],
      "metadata": {
        "id": "nH4a2Y7mL8F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "Y0uaymd8MRtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama"
      ],
      "metadata": {
        "id": "eH_ofrVqN3u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "qlHoHdctNte1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama"
      ],
      "metadata": {
        "id": "hRxOkAMHMi9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama2"
      ],
      "metadata": {
        "id": "l_TAry50MVrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama2\")"
      ],
      "metadata": {
        "id": "y4dFiVuFOaMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"how can langsmith help with testing?\")"
      ],
      "metadata": {
        "id": "T2QeQsgIOaVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are world class technical documentation writer.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ],
      "metadata": {
        "id": "42p6nnq1O0w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5W9Bhu50OaWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kf7eXiQuOaX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IK327S4lOaYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJg8-5_OOabT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEuHol_POacJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}